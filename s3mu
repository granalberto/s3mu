#!/usr/bin/env perl

# Copyright 2015 - Alberto Mijares <amijaresp**gmail*com>

use v5.10;
use warnings;
use strict;
use lib 'lib';
use S3mu::SQLite;
use S3mu::AmazonS3;
use S3mu::Utils;
use Getopt::Long;
use File::Spec;
use Parallel::ForkManager;
use Pod::Usage;


my $localdir = '';
my $dbfile = '';
my $resume = 0;
my $bucket = '';
my $remotedir = '';
my $ssl = 0;
my $nprocs = 5;
my $help;

GetOptions (
    'localdir=s' => \$localdir,
    'dbfile=s' => \$dbfile,
    'resume' => \$resume,
    'bucket=s' => \$bucket,
    'amazondir=s' => \$remotedir,
    'ssl' => \$ssl,
    'nprocs=i' => \$nprocs,
    'help' => \$help
    );


my $util = new S3mu::Utils;

my $sqlite = new S3mu::SQLite(dbfile => File::Spec->rel2abs($dbfile),
			      localdir => $localdir);

my $amazon = new S3mu::AmazonS3(bucket => $bucket,
				amazondir => $remotedir,
				ssl => $ssl,
				nprocs => $nprocs);

my $pm = Parallel::ForkManager->new($nprocs);

pod2usage(2) if $help;

if ($resume) {
    
    $util->file_exists($dbfile) || (pod2usage("\nAny problem with SQLite file?\n"));
    while (my $data = $sqlite->take_some($nprocs)) {
	for my $i (@$data) {
	    my $nstat;
	    $pm->start and next;
	    my $job = $amazon->upload($amazon->prepare($i->[1]));
	    $job == 1 ? $nstat = 2 : $nstat = 0;
	    $sqlite->set_stat($i->[0], $nstat);
	    $pm->finish;
	}
	$pm->wait_all_children;
    }
    say 'We are done here!';
    exit 0
}

$util->file_exists($dbfile) && pod2usage("\nProblems with SQLite file?\n");
$util->dir_exists($localdir) || pod2usage("\nProblems with local directory?\n");

$sqlite->create_db;
$sqlite->fill_db;
chdir $localdir;

while (my $data = $sqlite->take_some($nprocs)) {
       
    for my $i (@$data) {

	my $nstat;
	$pm->start and next;
	my $job = $amazon->upload($amazon->prepare($i->[1]));
	$job == 1 ? $nstat = 2 : $nstat = 0;
	$sqlite->set_stat($i->[0], $nstat);
	$pm->finish;
    }

    $pm->wait_all_children;
}
       
say 'We are done here!';

__END__

=encoding utf8

=head1 NAME

s3mu - AmazonS3 Massive and recursive file Uploader

=head1 SYNOPSIS

 -v  prints this help
 -n  number of concurrent uploads (forks)
 -l  local dir to upload 
 -d  database filename
 -b  bucket name
 -a  base dirname in AmazonS3 
 -s  Use SSL (optional)
 -r  resume (optional)

 Example:

 $ s3mu -n 20 -l ../project/images -d database.db -b images -a images/ -s

 Nota Bene: remember to set AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY

=cut
